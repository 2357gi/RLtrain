{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q - Learnの学習\n",
    "## SarsaとQの違い\n",
    "- Sarsaの行動価値関数Qの更新式\n",
    "$$\n",
    "    Q(s_t, a_t) = Q(s_t, a_t) + η*(R_{t+1} + μQ(s_{t+1}, a_{t+1}) - Q(s_t, a_t))\n",
    "$$\n",
    "\n",
    "- Q学習の行動価値関数Qの更新式\n",
    "$$\n",
    "Q(s_t, a_t) = Q(s_t, a_t) + η*(R_{t+1} + γ max_a Q(s_{t+1}, a) - Q(s_t, a_t)\n",
    "$$\n",
    "\n",
    "**Sarsaは$a_{t+1}$を更新に使用する**のに対して、**Q学習**は**a_{a+1}を更新に使用しない**ことが違いとして読み取れる。\n",
    "方策を行動価値関数の更新に用いるものを方策オン型(ON-policy)と呼び、方策を行動価値関数の更新に用いないものを方策オフ型(OFF-policy)と呼ぶ\n",
    "\n",
    "ε- Greedy法から生まれるランダム性が更新式に反映されない分、収束が早い(ε-greedy法は一定確率でランダムのaを取ることでコストを払い探索をしている。)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/2357gi/Sandbox/signatej/venv/lib/python3.7/site-packages/matplotlib/cbook/__init__.py:424: MatplotlibDeprecationWarning: \n",
      "Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warn_deprecated(\"2.2\", \"Passing one of 'on', 'true', 'off', 'false' as a \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 迷路の定義とpi_0の定義\n",
    "# 図を描く大きさと、図の変数を宣言。\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = plt.gca()\n",
    "\n",
    "# make wall\n",
    "plt.plot([1, 1], [0, 1], color='red', linewidth=2)\n",
    "plt.plot([1, 2], [2, 2], color='red', linewidth=2)\n",
    "plt.plot([2, 2], [2, 1], color='red', linewidth=2)\n",
    "plt.plot([2, 3], [1, 1], color='red', linewidth=2)\n",
    "\n",
    "# make points\n",
    "plt.text(0.5, 2.5, 'S0', size=14, ha='center')\n",
    "plt.text(1.5, 2.5, 'S1', size=14, ha='center')\n",
    "plt.text(2.5, 2.5, 'S2', size=14, ha='center')\n",
    "plt.text(0.5, 1.5, 'S3', size=14, ha='center')\n",
    "plt.text(1.5, 1.5, 'S4', size=14, ha='center')\n",
    "plt.text(2.5, 1.5, 'S5', size=14, ha='center')\n",
    "plt.text(0.5, 0.5, 'S6', size=14, ha='center')\n",
    "plt.text(1.5, 0.5, 'S7', size=14, ha='center')\n",
    "plt.text(2.5, 0.5, 'S8', size=14, ha='center')\n",
    "plt.text(0.5, 2.3, 'START', ha='center')\n",
    "plt.text(2.5, 0.3, 'GOAL', ha='center')\n",
    "\n",
    "# 描画範囲の設定とメモリを消す設定\n",
    "ax.set_xlim(0, 3)\n",
    "ax.set_ylim(0, 3)\n",
    "plt.tick_params(axis='both', which='both', bottom='off', top='off',\n",
    "                labelbottom='off', right='off', left='off', labelleft='off')\n",
    "\n",
    "# 現在地に丸を描画する。\n",
    "line, = ax.plot([0.5], [2.5], 'o', color = 'g', markersize=60)\n",
    "\n",
    "\n",
    "# 初期の方策を決定するparams theta_0を設定\n",
    "\n",
    "# 行は0~7, 列は移動方向で上右下左を表す。\n",
    "theta_0 = np.array([[np.nan, 1, 1, np.nan],  \n",
    "                   [np.nan, 1, np.nan, 1],\n",
    "                    [np.nan, np.nan, 1, 1],\n",
    "                   [1, 1, 1, np.nan],        \n",
    "                   [np.nan, np.nan, 1, 1],   \n",
    "                   [1, np.nan, np.nan, np.nan],  \n",
    "                   [1, np.nan, np.nan, np.nan],  \n",
    "                   [1, 1, np.nan, np.nan],       \n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.5       , 0.5       , 0.        ],\n",
       "       [0.        , 0.5       , 0.        , 0.5       ],\n",
       "       [0.        , 0.        , 0.5       , 0.5       ],\n",
       "       [0.33333333, 0.33333333, 0.33333333, 0.        ],\n",
       "       [0.        , 0.        , 0.5       , 0.5       ],\n",
       "       [1.        , 0.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.5       , 0.5       , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 方策パラメータtheta_0を方策に変形する。初期の方策なので、ランダム、よって割合で実装する。\n",
    "\n",
    "\n",
    "def simple_convert_into_pi_from_theta(theta):\n",
    "    \n",
    "    [m, n] = theta.shape\n",
    "    pi = np.zeros((m, n))\n",
    "    for i in range(0, m):\n",
    "        pi[i, :] = theta[i, :] / np.nansum(theta[i, :])\n",
    "    \n",
    "    pi = np.where(np.isnan(pi), 0, pi)\n",
    "    \n",
    "    return pi\n",
    "\n",
    "pi_0 = simple_convert_into_pi_from_theta(theta_0)\n",
    "pi_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期の行動価値関数Qを設定。\n",
    "# 最初はランダムでパラメータを定義、theta_0をかけてnanを反映する。\n",
    "\n",
    "[a, b] = theta_0.shape\n",
    "Q = np.random.rand(a, b) * theta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ε-greedy法を実装\n",
    "\n",
    "\n",
    "def get_action(s, Q, epsilon, pi_0):\n",
    "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
    "\n",
    "    # 行動を決める\n",
    "    if np.random.rand() < epsilon:\n",
    "        # εの確率でランダムに動く\n",
    "        next_direction = np.random.choice(direction, p=pi_0[s, :])\n",
    "    else:\n",
    "        # Qの最大値の行動を採用する\n",
    "        next_direction = direction[np.nanargmax(Q[s, :])]\n",
    "\n",
    "    # 行動をindexに\n",
    "    if next_direction == \"up\":\n",
    "        action = 0\n",
    "    elif next_direction == \"right\":\n",
    "        action = 1\n",
    "    elif next_direction == \"down\":\n",
    "        action = 2\n",
    "    elif next_direction == \"left\":\n",
    "        action = 3\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def get_s_next(s, a, Q, epsilon, pi_0):\n",
    "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
    "    next_direction = direction[a]  # 行動aの方向\n",
    "\n",
    "    # 行動から次の状態を決める\n",
    "    if next_direction == \"up\":\n",
    "        s_next = s - 3  # 上に移動するときは状態の数字が3小さくなる\n",
    "    elif next_direction == \"right\":\n",
    "        s_next = s + 1  # 右に移動するときは状態の数字が1大きくなる\n",
    "    elif next_direction == \"down\":\n",
    "        s_next = s + 3  # 下に移動するときは状態の数字が3大きくなる\n",
    "    elif next_direction == \"left\":\n",
    "        s_next = s - 1  # 左に移動するときは状態の数字が1小さくなる\n",
    "\n",
    "    return s_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q学習の実装\n",
    "\n",
    "\n",
    "def Q_learning_old(s, a, r, s_next, Q, eta, gamma):\n",
    "    \n",
    "    if s_next == 8:\n",
    "        Q[s, a] = Q[s, a] + eta * (r - Q[s, a])\n",
    "    else:\n",
    "        Q[s, a] = Q[s, a] + eta * (r + gamma + np.nanmax(Q[s_next, :] - Q[s, a]))\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q学習による行動価値関数Qの更新\n",
    "\n",
    "\n",
    "def Q_learning(s, a, r, s_next, Q, eta, gamma):\n",
    "\n",
    "    if s_next == 8:  # ゴールした場合\n",
    "        Q[s, a] = Q[s, a] + eta * (r - Q[s, a])\n",
    "\n",
    "    else:\n",
    "        Q[s, a] = Q[s, a] + eta * (r + gamma * np.nanmax(Q[s_next,: ]) - Q[s, a])\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q学習で迷路を解く関数の定義。１試行ごとに実行される。というか試行そのもの\n",
    "def goal_maze_rets_s_a_Q_old(Q, epsilon, eta, gamma, pi):\n",
    "    s = 0  # スタート地点\n",
    "    a = a_next = get_action(s, Q, epsilon, pi)  # 状態の行動\n",
    "    s_a_history = [[0, np.nan]]  # エージェントの行動履歴\n",
    "    \n",
    "    while(1):  # ゴールするまでループ\n",
    "        a = a_next  # 行動更新\n",
    "        \n",
    "        s_a_history[-1][1] = a\n",
    "        # 現在の状態(一番最後なのでindexは-1を見てる)に行動を代入\n",
    "        \n",
    "        s_next = get_s_next(s, a, Q, epsilon, pi)\n",
    "        # 次の行動を格納。s_t\n",
    "        \n",
    "        s_a_history.append([s_next, np.nan])\n",
    "        # 次の状態を格納。\n",
    "        \n",
    "        if s_next == 8:\n",
    "            r = 1  # ゴールにたどり着いたら報酬を与える。\n",
    "            a_next = np.nan\n",
    "        else:\n",
    "            r = 0\n",
    "            a_next = get_action(s_next, Q, epsilon, pi)\n",
    "            # 次の行動関数a_nextを求める。\n",
    "        \n",
    "        # 価値関数を更新\n",
    "        Q = Q_learning(s, a, r, s_next, Q, eta, gamma)\n",
    "        \n",
    "        if s_next == 8:\n",
    "            break\n",
    "        else:\n",
    "            s = s_next\n",
    "    return [s_a_history, Q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi):\n",
    "    s = 0  # スタート地点\n",
    "    a = a_next = get_action(s, Q, epsilon, pi)  # 初期の行動\n",
    "    s_a_history = [[0, np.nan]]  # エージェントの移動を記録するリスト\n",
    "\n",
    "    while (1):  # ゴールするまでループ\n",
    "        a = a_next  # 行動更新\n",
    "\n",
    "        s_a_history[-1][1] = a\n",
    "        # 現在の状態（つまり一番最後なのでindex=-1）に行動を代入\n",
    "\n",
    "        s_next = get_s_next(s, a, Q, epsilon, pi)\n",
    "        # 次の状態を格納\n",
    "\n",
    "        s_a_history.append([s_next, np.nan])\n",
    "        # 次の状態を代入。行動はまだ分からないのでnanにしておく\n",
    "\n",
    "        # 報酬を与え,　次の行動を求めます\n",
    "        if s_next == 8:\n",
    "            r = 1  # ゴールにたどり着いたなら報酬を与える\n",
    "            a_next = np.nan\n",
    "        else:\n",
    "            r = 0\n",
    "            a_next = get_action(s_next, Q, epsilon, pi)\n",
    "            # 次の行動a_nextを求めます。\n",
    "\n",
    "        # 価値関数を更新\n",
    "        Q = Q_learning(s, a, r, s_next, Q, eta, gamma)\n",
    "\n",
    "        # 終了判定\n",
    "        if s_next == 8:  # ゴール地点なら終了\n",
    "            break\n",
    "        else:\n",
    "            s = s_next\n",
    "\n",
    "    return [s_a_history, Q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------episode: 1\n",
      "--------------------行動価値の最大値\n",
      "14517.490160220521\n",
      "--------------------迷路を解くのにかかったステップ数は16です。\n",
      "--------------------episode: 2\n",
      "--------------------行動価値の最大値\n",
      "134378.20708019924\n",
      "--------------------迷路を解くのにかかったステップ数は154です。\n",
      "--------------------episode: 3\n",
      "--------------------行動価値の最大値\n",
      "308672.4015716547\n",
      "--------------------迷路を解くのにかかったステップ数は1058です。\n",
      "--------------------episode: 4\n",
      "--------------------行動価値の最大値\n",
      "54518.991961723244\n",
      "--------------------迷路を解くのにかかったステップ数は546です。\n",
      "--------------------episode: 5\n",
      "--------------------行動価値の最大値\n",
      "36670.91927834283\n",
      "--------------------迷路を解くのにかかったステップ数は950です。\n",
      "--------------------episode: 6\n",
      "--------------------行動価値の最大値\n",
      "17784.31416147358\n",
      "--------------------迷路を解くのにかかったステップ数は1520です。\n",
      "--------------------episode: 7\n",
      "--------------------行動価値の最大値\n",
      "4846.476239093787\n",
      "--------------------迷路を解くのにかかったステップ数は6622です。\n",
      "--------------------episode: 8\n",
      "--------------------行動価値の最大値\n",
      "0.11131356812707693\n",
      "--------------------迷路を解くのにかかったステップ数は20です。\n",
      "--------------------episode: 9\n",
      "--------------------行動価値の最大値\n",
      "0.07297936681902173\n",
      "--------------------迷路を解くのにかかったステップ数は16です。\n",
      "--------------------episode: 10\n",
      "--------------------行動価値の最大値\n",
      "0.08449180742280338\n",
      "--------------------迷路を解くのにかかったステップ数は16です。\n",
      "--------------------episode: 11\n",
      "--------------------行動価値の最大値\n",
      "0.04168870225944343\n",
      "--------------------迷路を解くのにかかったステップ数は10です。\n",
      "--------------------episode: 12\n",
      "--------------------行動価値の最大値\n",
      "0.06301230596226781\n",
      "--------------------迷路を解くのにかかったステップ数は14です。\n",
      "--------------------episode: 13\n",
      "--------------------行動価値の最大値\n",
      "0.03651007903282777\n",
      "--------------------迷路を解くのにかかったステップ数は8です。\n",
      "--------------------episode: 14\n",
      "--------------------行動価値の最大値\n",
      "0.038733404693587214\n",
      "--------------------迷路を解くのにかかったステップ数は10です。\n",
      "--------------------episode: 15\n",
      "--------------------行動価値の最大値\n",
      "0.04545811171052594\n",
      "--------------------迷路を解くのにかかったステップ数は12です。\n",
      "--------------------episode: 16\n",
      "--------------------行動価値の最大値\n",
      "0.023059016733355198\n",
      "--------------------迷路を解くのにかかったステップ数は6です。\n",
      "--------------------episode: 17\n",
      "--------------------行動価値の最大値\n",
      "0.0358913829851234\n",
      "--------------------迷路を解くのにかかったステップ数は10です。\n",
      "--------------------episode: 18\n",
      "--------------------行動価値の最大値\n",
      "0.036473806250113094\n",
      "--------------------迷路を解くのにかかったステップ数は10です。\n",
      "--------------------episode: 19\n",
      "--------------------行動価値の最大値\n",
      "0.02371468835790258\n",
      "--------------------迷路を解くのにかかったステップ数は8です。\n",
      "--------------------episode: 20\n",
      "--------------------行動価値の最大値\n",
      "0.032359962110412144\n",
      "--------------------迷路を解くのにかかったステップ数は8です。\n",
      "--------------------episode: 21\n",
      "--------------------行動価値の最大値\n",
      "0.006088940450285518\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 22\n",
      "--------------------行動価値の最大値\n",
      "0.032881749455548004\n",
      "--------------------迷路を解くのにかかったステップ数は10です。\n",
      "--------------------episode: 23\n",
      "--------------------行動価値の最大値\n",
      "0.017502563526766157\n",
      "--------------------迷路を解くのにかかったステップ数は6です。\n",
      "--------------------episode: 24\n",
      "--------------------行動価値の最大値\n",
      "0.018216649583003508\n",
      "--------------------迷路を解くのにかかったステップ数は8です。\n",
      "--------------------episode: 25\n",
      "--------------------行動価値の最大値\n",
      "0.02220809188870665\n",
      "--------------------迷路を解くのにかかったステップ数は8です。\n",
      "--------------------episode: 26\n",
      "--------------------行動価値の最大値\n",
      "0.013791158084747512\n",
      "--------------------迷路を解くのにかかったステップ数は6です。\n",
      "--------------------episode: 27\n",
      "--------------------行動価値の最大値\n",
      "0.01749821844469468\n",
      "--------------------迷路を解くのにかかったステップ数は6です。\n",
      "--------------------episode: 28\n",
      "--------------------行動価値の最大値\n",
      "0.003921580249335643\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 29\n",
      "--------------------行動価値の最大値\n",
      "0.016003622104365234\n",
      "--------------------迷路を解くのにかかったステップ数は6です。\n",
      "--------------------episode: 30\n",
      "--------------------行動価値の最大値\n",
      "0.025455387807975627\n",
      "--------------------迷路を解くのにかかったステップ数は10です。\n",
      "--------------------episode: 31\n",
      "--------------------行動価値の最大値\n",
      "0.005266206932678208\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 32\n",
      "--------------------行動価値の最大値\n",
      "0.004225279774250135\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 33\n",
      "--------------------行動価値の最大値\n",
      "0.01316021058723782\n",
      "--------------------迷路を解くのにかかったステップ数は6です。\n",
      "--------------------episode: 34\n",
      "--------------------行動価値の最大値\n",
      "0.003603549966404551\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 35\n",
      "--------------------行動価値の最大値\n",
      "0.013194959109901538\n",
      "--------------------迷路を解くのにかかったステップ数は6です。\n",
      "--------------------episode: 36\n",
      "--------------------行動価値の最大値\n",
      "0.0018353314666256226\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 37\n",
      "--------------------行動価値の最大値\n",
      "0.013343524429618725\n",
      "--------------------迷路を解くのにかかったステップ数は6です。\n",
      "--------------------episode: 38\n",
      "--------------------行動価値の最大値\n",
      "0.005282359131834835\n",
      "--------------------迷路を解くのにかかったステップ数は6です。\n",
      "--------------------episode: 39\n",
      "--------------------行動価値の最大値\n",
      "0.010772010427177126\n",
      "--------------------迷路を解くのにかかったステップ数は6です。\n",
      "--------------------episode: 40\n",
      "--------------------行動価値の最大値\n",
      "0.0029006480626575515\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 41\n",
      "--------------------行動価値の最大値\n",
      "0.0021880208049573913\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 42\n",
      "--------------------行動価値の最大値\n",
      "0.011010852076355215\n",
      "--------------------迷路を解くのにかかったステップ数は6です。\n",
      "--------------------episode: 43\n",
      "--------------------行動価値の最大値\n",
      "0.0030458584213898243\n",
      "--------------------迷路を解くのにかかったステップ数は6です。\n",
      "--------------------episode: 44\n",
      "--------------------行動価値の最大値\n",
      "0.009958367925990808\n",
      "--------------------迷路を解くのにかかったステップ数は6です。\n",
      "--------------------episode: 45\n",
      "--------------------行動価値の最大値\n",
      "0.002095783062330492\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 46\n",
      "--------------------行動価値の最大値\n",
      "0.001955634593226785\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 47\n",
      "--------------------行動価値の最大値\n",
      "0.0018239152986512241\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 48\n",
      "--------------------行動価値の最大値\n",
      "0.0017002047849947743\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 49\n",
      "--------------------行動価値の最大値\n",
      "0.0015840960811913662\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 50\n",
      "--------------------行動価値の最大値\n",
      "0.0005420117160976901\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 51\n",
      "--------------------行動価値の最大値\n",
      "0.009762824180597929\n",
      "--------------------迷路を解くのにかかったステップ数は6です。\n",
      "--------------------episode: 52\n",
      "--------------------行動価値の最大値\n",
      "0.001277515459819134\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 53\n",
      "--------------------行動価値の最大値\n",
      "0.00118801743381447\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 54\n",
      "--------------------行動価値の最大値\n",
      "0.0011042918945770586\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 55\n",
      "--------------------行動価値の最大値\n",
      "0.0010260143720435266\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 56\n",
      "--------------------行動価値の最大値\n",
      "0.0008912250678666522\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 57\n",
      "--------------------行動価値の最大値\n",
      "0.009002139291729327\n",
      "--------------------迷路を解くのにかかったステップ数は6です。\n",
      "--------------------episode: 58\n",
      "--------------------行動価値の最大値\n",
      "0.0008213843309586855\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 59\n",
      "--------------------行動価値の最大値\n",
      "0.0007618698701600257\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 60\n",
      "--------------------行動価値の最大値\n",
      "0.00039871653495282544\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 61\n",
      "--------------------行動価値の最大値\n",
      "0.008290488372548532\n",
      "--------------------迷路を解くのにかかったステップ数は6です。\n",
      "--------------------episode: 62\n",
      "--------------------行動価値の最大値\n",
      "0.0006065526552686551\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 63\n",
      "--------------------行動価値の最大値\n",
      "0.0005617432991651405\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 64\n",
      "--------------------行動価値の最大値\n",
      "0.0005200556653428556\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 65\n",
      "--------------------行動価値の最大値\n",
      "0.0004812909130692544\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 66\n",
      "--------------------行動価値の最大値\n",
      "0.0004452610167262616\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 67\n",
      "--------------------行動価値の最大値\n",
      "0.00041178836425603826\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 68\n",
      "--------------------行動価値の最大値\n",
      "0.00038070535051026866\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 69\n",
      "--------------------行動価値の最大値\n",
      "0.0003518539682616506\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 70\n",
      "--------------------行動価値の最大値\n",
      "0.0003250853993588265\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 71\n",
      "--------------------行動価値の最大値\n",
      "0.000256004945248689\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 72\n",
      "--------------------行動価値の最大値\n",
      "0.0005034178139499135\n",
      "--------------------迷路を解くのにかかったステップ数は6です。\n",
      "--------------------episode: 73\n",
      "--------------------行動価値の最大値\n",
      "0.00023772590192328025\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 74\n",
      "--------------------行動価値の最大値\n",
      "0.00021978924382570764\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 75\n",
      "--------------------行動価値の最大値\n",
      "0.00020313318447473527\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 76\n",
      "--------------------行動価値の最大値\n",
      "0.00018767390174556642\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 77\n",
      "--------------------行動価値の最大値\n",
      "0.00017333224048032836\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 78\n",
      "--------------------行動価値の最大値\n",
      "0.00016003354630023114\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 79\n",
      "--------------------行動価値の最大値\n",
      "0.0001477074931024447\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 80\n",
      "--------------------行動価値の最大値\n",
      "0.0001362879064552569\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 81\n",
      "--------------------行動価値の最大値\n",
      "0.00012571258479332492\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 82\n",
      "--------------------行動価値の最大値\n",
      "0.00011592312004227168\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 83\n",
      "--------------------行動価値の最大値\n",
      "0.00010686471905618777\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 84\n",
      "--------------------行動価値の最大値\n",
      "9.848602702833276e-05\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 85\n",
      "--------------------行動価値の最大値\n",
      "9.073895384958952e-05\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 86\n",
      "--------------------行動価値の最大値\n",
      "8.357850420903645e-05\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 87\n",
      "--------------------行動価値の最大値\n",
      "7.696261208500754e-05\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 88\n",
      "--------------------行動価値の最大値\n",
      "7.085198014245009e-05\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 89\n",
      "--------------------行動価値の最大値\n",
      "6.520992443270757e-05\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 90\n",
      "--------------------行動価値の最大値\n",
      "6.0002224693156414e-05\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 91\n",
      "--------------------行動価値の最大値\n",
      "5.51969804596375e-05\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 92\n",
      "--------------------行動価値の最大値\n",
      "5.076447311957999e-05\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 93\n",
      "--------------------行動価値の最大値\n",
      "4.667703398220091e-05\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 94\n",
      "--------------------行動価値の最大値\n",
      "4.290891837310795e-05\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 95\n",
      "--------------------行動価値の最大値\n",
      "3.9436185727437234e-05\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 96\n",
      "--------------------行動価値の最大値\n",
      "3.623658560392151e-05\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 97\n",
      "--------------------行動価値の最大値\n",
      "3.328944951830337e-05\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 98\n",
      "--------------------行動価値の最大値\n",
      "3.057558846830677e-05\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 99\n",
      "--------------------行動価値の最大値\n",
      "2.8077195991516035e-05\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n",
      "--------------------episode: 100\n",
      "--------------------行動価値の最大値\n",
      "2.5777756594402845e-05\n",
      "--------------------迷路を解くのにかかったステップ数は4です。\n"
     ]
    }
   ],
   "source": [
    "# Q学習で迷路を解く\n",
    "\n",
    "eta = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.5\n",
    "v = np.nanmax(Q, axis=1)\n",
    "is_continue = True\n",
    "episode = 1\n",
    "\n",
    "V = []\n",
    "V.append(np.nanmax(Q, axis=1))\n",
    "\n",
    "while is_continue:\n",
    "    print(\"--\"*10 + \"episode: \" + str(episode))\n",
    "    \n",
    "    # ε-greedyのepsilonを徐々に小さくする\n",
    "    epsilon = epsilon / 2\n",
    "    \n",
    "    # Q学習で迷路を解き、移動した履歴と更新したQを求める。\n",
    "    [s_a_history, Q] = goal_maze_rets_s_a_Q(Q, epsilon, eta, gamma, pi_0)\n",
    "    \n",
    "    # 状態価値の変化\n",
    "    new_v = np.nanmax(Q, axis=1)\n",
    "    print(\"--\"*10 + \"行動価値の最大値\\n\" + str(np.sum(np.abs(new_v - v))))\n",
    "    v = new_v\n",
    "    V.append(v)\n",
    "    \n",
    "    print(\"--\"*10 + \"迷路を解くのにかかったステップ数は\"+str(len(s_a_history) - 1) + \"です。\")\n",
    "    \n",
    "    \n",
    "    # 100ep繰り返す\n",
    "    episode = episode + 1\n",
    "    if episode > 100:\n",
    "        is_continue = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
